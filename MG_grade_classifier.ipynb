{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513df175-0660-4f34-ac72-815568630bfb",
   "metadata": {},
   "source": [
    "# Creation of Meningioma Grade Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f961478-0bfb-413f-a452-144697493a1b",
   "metadata": {},
   "source": [
    "## Data Read-in and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b7a8b-91a9-4297-8398-7a325b20c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# load the dataset \n",
    "\n",
    "b_cancer = pd.read_csv('Dataset _01.csv')\n",
    "\n",
    "# data cleaning \n",
    "\n",
    "# inspect data\n",
    "\n",
    "b_cancer.shape\n",
    "\n",
    "# Inspect: number of samples and number of samples\n",
    "print(str(\"dataset has \") + str(b_cancer.shape[0])+str(' samples / instances & ')+str(b_cancer.shape[1])+ str(' features'))\n",
    "b_cancer.head(10)\n",
    "b_cancer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9cc45a-ee3e-4215-b671-9e108dbecd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of missing values for each feature\n",
    "\n",
    "b_cancer.isna().sum()\n",
    "\n",
    "# no missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ce621-c845-4744-9bc7-6b4679c770b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for uplicate rows \n",
    "\n",
    "duplicate_rows = b_cancer[b_cancer.duplicated()]\n",
    "print(duplicate_rows) # empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd2721-89c8-4acb-aaf3-d356d298bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data types \n",
    "\n",
    "print(b_cancer.dtypes) #int64 is sufficient for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cc308-a207-4503-b382-36994e9a7550",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb0625-a77d-400c-a84b-9d87c42dcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_canc = b_cancer.describe().T\n",
    "stats_canc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd7f2e-d030-46c1-b674-7bc7a78697d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for grade 1\n",
    "\n",
    "grade_one = b_cancer[b_cancer['Grade']==0].describe().T\n",
    "grade_one.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8d5ac-5b25-459b-856e-52441c5617ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for grade 2 \n",
    "\n",
    "grade_two = b_cancer[b_cancer['Grade']==1].describe().T\n",
    "grade_two.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b997b47-23cd-420b-b62c-d70df9231cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare fold change \n",
    "# create dataframe\n",
    "f_means = pd.DataFrame({'Grade 1':grade_one[:-1]['mean'], 'Grade 2':grade_two[:-1]['mean']})\n",
    "\n",
    "# calculate fold changes and add to dataframe\n",
    "fc = (f_means['Grade 1'] - f_means['Grade 2']) / f_means['Grade 2']\n",
    "f_means['fc'] = fc\n",
    "\n",
    "# calculate Log2 of fold change and add to data frame\n",
    "log2_fc = np.log2(np.abs(fc))\n",
    "f_means['log2_fc'] = log2_fc\n",
    "f_means = f_means.reset_index().rename(columns = {'index':'feature'})\n",
    "\n",
    "# Drop the 'Grade' row before sorting\n",
    "f_means = f_means[f_means['feature'] != 'Grade']\n",
    "\n",
    "# sort dataframe according to fold change\n",
    "f_means['abs'] = abs(f_means['fc'])\n",
    "\n",
    "# Sort by fold absolute change\n",
    "f_means = f_means.sort_values(by = ['abs'],ascending = False).drop(columns = ['abs'])\n",
    "f_means.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afe930-c358-4132-99cc-60f722590dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot log fold changes \n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(f_means['feature'], f_means['log2_fc'], color='blue', width=0.4)\n",
    "plt.xticks(f_means['feature'], rotation='vertical')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Log2 Fold change')  # Updated y-axis label\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244147b0-4766-4324-a723-233dd12d6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot top 4\n",
    "# Extract top ten variables from f_means\n",
    "top_variables = f_means.head(10)['feature']\n",
    "\n",
    "# Select columns corresponding to top variables from b_cancer\n",
    "top_variables_data = b_cancer[['Grade'] + top_variables.tolist()]\n",
    "\n",
    "# Set seaborn plotting aesthetics as default\n",
    "sns.set()\n",
    "\n",
    "# Define plotting region (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Create box plot in each subplot\n",
    "sns.boxplot(x='Grade', y=top_variables_data[top_variables.tolist()[0]], data=top_variables_data, ax=axes[0, 0])\n",
    "sns.boxplot(x='Grade', y=top_variables_data[top_variables.tolist()[1]], data=top_variables_data, ax=axes[0, 1])\n",
    "sns.boxplot(x='Grade', y=top_variables_data[top_variables.tolist()[2]], data=top_variables_data, ax=axes[1, 0])\n",
    "sns.boxplot(x='Grade', y=top_variables_data[top_variables.tolist()[3]], data=top_variables_data, ax=axes[1, 1])\n",
    "\n",
    "# Amend x-axis tick labels\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticklabels(['Grade 1', 'Grade 2'])\n",
    "\n",
    "# Set labels and title\n",
    "for ax, feature in zip(axes.flatten(), top_variables):\n",
    "    ax.set_xlabel('Grade')\n",
    "    ax.set_ylabel(feature)\n",
    "\n",
    "# Add a single title above all subplots\n",
    "fig.suptitle('Box Plots of Top log2 FC Variables against Grade', fontsize=16)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa6a9b-ced7-41dc-9d82-a164ac32bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plots\n",
    "# Extract top ten variables from f_means\n",
    "top_variables = f_means.head(10)['feature']\n",
    "\n",
    "# Select columns corresponding to top variables from b_cancer\n",
    "top_variables_data = b_cancer[['Grade'] + top_variables.tolist()]\n",
    "\n",
    "# Set seaborn plotting aesthetics as default\n",
    "sns.set()\n",
    "\n",
    "# Define plotting region (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Create violin plot in each subplot\n",
    "sns.violinplot(x='Grade', y=top_variables_data[top_variables.tolist()[0]], data=top_variables_data, ax=axes[0, 0])\n",
    "sns.violinplot(x='Grade', y=top_variables_data[top_variables.tolist()[1]], data=top_variables_data, ax=axes[0, 1])\n",
    "sns.violinplot(x='Grade', y=top_variables_data[top_variables.tolist()[2]], data=top_variables_data, ax=axes[1, 0])\n",
    "sns.violinplot(x='Grade', y=top_variables_data[top_variables.tolist()[3]], data=top_variables_data, ax=axes[1, 1])\n",
    "\n",
    "# Amend x-axis tick labels\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticklabels(['Grade 1', 'Grade 2'])\n",
    "\n",
    "# Set labels and title\n",
    "for ax, feature in zip(axes.flatten(), top_variables):\n",
    "    ax.set_xlabel('Grade')\n",
    "    ax.set_ylabel(feature)\n",
    "\n",
    "# Add a single title above all subplots\n",
    "fig.suptitle('Violin Plots of Top log2 FC Variables against Grade', fontsize=16)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cc5f3-3b38-4dac-8db6-75e94cb518a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swarm plots\n",
    "# Extract top ten variables from f_means\n",
    "top_variables = f_means.head(10)['feature']\n",
    "\n",
    "# Select columns corresponding to top variables from b_cancer\n",
    "top_variables_data = b_cancer[['Grade'] + top_variables.tolist()]\n",
    "\n",
    "# Set seaborn plotting aesthetics as default\n",
    "sns.set()\n",
    "\n",
    "# Define plotting region (2 rows, 2 columns)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Create swarm plot in each subplot\n",
    "sns.swarmplot(x='Grade', y=top_variables_data[top_variables.tolist()[0]], data=top_variables_data, ax=axes[0, 0], hue='Grade', palette='Set1')\n",
    "sns.swarmplot(x='Grade', y=top_variables_data[top_variables.tolist()[1]], data=top_variables_data, ax=axes[0, 1], hue='Grade', palette='Set1')\n",
    "sns.swarmplot(x='Grade', y=top_variables_data[top_variables.tolist()[2]], data=top_variables_data, ax=axes[1, 0], hue='Grade', palette='Set1')\n",
    "sns.swarmplot(x='Grade', y=top_variables_data[top_variables.tolist()[3]], data=top_variables_data, ax=axes[1, 1], hue='Grade', palette='Set1')\n",
    "\n",
    "# Amend x-axis tick labels\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticklabels(['Grade 1', 'Grade 2'])\n",
    "    # remove legend because i've updated x axis labels\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Set labels and title\n",
    "for ax, feature in zip(axes.flatten(), top_variables):\n",
    "    ax.set_xlabel('Grade')\n",
    "    ax.set_ylabel(feature)\n",
    "\n",
    "# Add a single title above all subplots\n",
    "fig.suptitle('Swarm Plots of Top log2 FC Variables against Grade', fontsize=16)\n",
    "\n",
    "# Adjust legend location\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e17395-52ce-4e09-8f3a-52823c460d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if features are normally distributed or not\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# list features \n",
    "features = [col for col in b_cancer.columns if col not in['Grade', 'Subjects']]\n",
    "\n",
    "# Loop through each feature\n",
    "for feature in features:\n",
    "    # Extract the data for the current feature\n",
    "    data = b_cancer[feature]\n",
    "    \n",
    "    # Perform normality test\n",
    "    stat, p = stats.normaltest(data)\n",
    "    \n",
    "    # Set significance level\n",
    "    alpha = 0.05\n",
    "    \n",
    "    # Print the result\n",
    "    print(f'Feature: {feature}')\n",
    "    if p > alpha:\n",
    "        print('  Data is normally distributed')\n",
    "    else:\n",
    "        print('  Data is not normally distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa94ca9-0f6e-4451-886d-a6d30eb3c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## t-test or mann-whitney tests for comparison \n",
    "# List features to perform tests on  \n",
    "features = [col for col in b_cancer.columns if col not in ['Grade', 'Subjects']]\n",
    "\n",
    "# Initialize lists to store normally and not normally distributed features\n",
    "normally_distributed_features = []\n",
    "not_normally_distributed_features = []\n",
    "results = []\n",
    "\n",
    "# Loop through each feature\n",
    "for feature in features:\n",
    "    # Extract the data for the current feature\n",
    "    data = b_cancer[feature]\n",
    "    \n",
    "    # Perform normality test\n",
    "    stat, p = stats.normaltest(data)\n",
    "    \n",
    "    # Set significance level\n",
    "    alpha = 0.05\n",
    "    \n",
    "    # Determine whether the data is normally distributed and perform the corresponding test\n",
    "    if p > alpha:\n",
    "        normally_distributed_features.append(feature)\n",
    "        grade = 1  # Choose either 0 or 1, as per your preference\n",
    "        group1 = b_cancer[b_cancer['Grade'] == grade][feature]\n",
    "        group2 = b_cancer[b_cancer['Grade'] != grade][feature]\n",
    "        t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "        results.append({'Feature': feature, 'Test': 'T-test', 'Statistic': t_stat, 'P-value': p_value})\n",
    "    else:\n",
    "        not_normally_distributed_features.append(feature)\n",
    "        grade = 1  # Choose either 0 or 1, as per your preference\n",
    "        group1 = b_cancer[b_cancer['Grade'] == grade][feature]\n",
    "        group2 = b_cancer[b_cancer['Grade'] != grade][feature]\n",
    "        u_stat, p_value = stats.mannwhitneyu(group1, group2)\n",
    "        results.append({'Feature': feature, 'Test': 'Mann-Whitney U test', 'Statistic': u_stat, 'P-value': p_value})\n",
    "\n",
    "# Convert results list into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf2eee-7a96-4d17-a9ab-13c7f752cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROC with AUC and append it to results_df\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Iterate over each continuous predictor variable\n",
    "for feature in features:\n",
    "    # Perform logistic regression with the predictor variable\n",
    "    X = b_cancer[[feature]]\n",
    "    y = b_cancer['Grade']\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calculate predicted probabilities\n",
    "    predicted_probabilities = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate the AUC\n",
    "    auc = roc_auc_score(y, predicted_probabilities)\n",
    "    \n",
    "    # Find the corresponding row and append the AUC value\n",
    "    index = results_df[results_df['Feature'] == feature].index\n",
    "    if not index.empty:\n",
    "        results_df.loc[index, 'AUC'] = auc\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1da99-9465-42c1-b300-cf2507cf951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point biserial correlation of features with grade\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Extract continuous variables excluding 'Grade' and 'Subjects'\n",
    "continuous_vars = b_cancer.drop(columns=['Grade', 'Subjects'])\n",
    "\n",
    "# Calculate point-biserial correlation between binary 'Grade' and continuous variables\n",
    "point_biserial_correlation = {}\n",
    "for col in continuous_vars.columns:\n",
    "    corr_coef, p_value = pointbiserialr(b_cancer[col], b_cancer['Grade'])\n",
    "    point_biserial_correlation[col] = corr_coef\n",
    "\n",
    "# Convert the dictionary to a DataFrame for visualization\n",
    "point_biserial_df = pd.DataFrame.from_dict(point_biserial_correlation, orient='index', columns=['Point-Biserial Correlation'])\n",
    "\n",
    "# transpose df to make horizontal graph\n",
    "point_biserial_df = point_biserial_df.transpose()\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(14, 10))  # Increase the figure size\n",
    "heatmap = sns.heatmap(point_biserial_df.transpose(), cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "plt.xticks(fontsize=10)  # Rotate x-axis labels, adjust alignment, and font size\n",
    "plt.yticks(rotation=0, ha='right', fontsize=10)  # Adjust y-axis font size\n",
    "plt.title('Point-Biserial Correlation Heatmap of Continuous Variables with Grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca2256-c489-4757-aff3-ad0fec66e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of just the predictor variables to look for confounding variables \n",
    "unscaled_correlation_matrix = continuous_vars.corr()\n",
    "\n",
    "# Create a heatmap with adjusted x-axis and y-axis tick labels\n",
    "plt.figure(figsize=(20, 12))\n",
    "heatmap = sns.heatmap(unscaled_correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)  # Rotate x-axis labels by 90 degrees, adjust alignment, and font size\n",
    "plt.yticks(rotation=0, ha='right', fontsize=8)   # Adjust y-axis font size and alignment\n",
    "plt.title('Correlation Heatmap of Continuous Variables')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355d634-db4c-4d5f-9fcf-c9e5bb211720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create treated data set\n",
    "# dimensionality reduction based on colinearity \n",
    "\n",
    "# Set colinearity threshold    \n",
    "correlation_threshold = 0.80        \n",
    "\n",
    "# Extract just the data without 'Subjects' and 'Grade'\n",
    "data = b_cancer.drop(columns=['Subjects', 'Grade'])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix -- inter-correlations or colinearity\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Determine features that have a colinearity above threshold\n",
    "# Need to use the absolute value -- to determine colinearity\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "# Drop the identified columns from b_cancer\n",
    "b_cancer_treated = b_cancer.drop(columns=to_drop) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431316b-7bf8-4577-89a1-bc1d293097c1",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2730c71-95a4-492c-962d-e710c476370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numeric features excluding 'Subjects' and 'Grade'\n",
    "numeric_features = b_cancer.drop(columns=['Subjects', 'Grade']).select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the numeric features\n",
    "scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Create a DataFrame with scaled numeric features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)\n",
    "\n",
    "# Concatenate scaled numeric features with 'Grade' column\n",
    "b_cancer = pd.concat([scaled_df, b_cancer['Grade']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b839990-405e-40b1-bb5e-e97f9ac2f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for treated data for valid comparison\n",
    "# Select numeric features excluding 'Subjects' and 'Grade'\n",
    "numeric_features = b_cancer_treated.drop(columns=['Subjects', 'Grade']).select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the numeric features\n",
    "scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Create a DataFrame with scaled numeric features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)\n",
    "\n",
    "# Concatenate scaled numeric features with 'Grade' column\n",
    "b_cancer_treated = pd.concat([scaled_df, b_cancer_treated['Grade']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb7afd-46b2-4cf9-ab45-b099282b8429",
   "metadata": {},
   "source": [
    "## Outlier Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25f8b1-ef0a-457f-bf4e-de5b90c7e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outliers\n",
    "# here's how I found rows that were in the 2.5% of outliers\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# List of features to exclude from outlier detection\n",
    "exclude_features = ['Subjects', 'Grade']  # Specify the features to exclude\n",
    "\n",
    "# Identify float columns and exclude specific features\n",
    "float_columns = b_cancer.select_dtypes(include=['float']).columns\n",
    "float_columns = [col for col in float_columns if col not in exclude_features]\n",
    "\n",
    "# Subset the DataFrame to include only the selected float columns\n",
    "float_data = b_cancer[float_columns]\n",
    "\n",
    "# Instantiate and fit LOF model\n",
    "lof_model = LocalOutlierFactor(contamination=0.025)  # toggle contam\n",
    "outlier_scores = lof_model.fit_predict(float_data)\n",
    "\n",
    "# Identify outliers\n",
    "outliers = b_cancer.iloc[outlier_scores == -1]\n",
    "\n",
    "# Compute correlations between each feature and their outlier scores\n",
    "correlations = float_data.corrwith(pd.Series(outlier_scores))\n",
    "\n",
    "# Sort correlations by absolute values\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35513f82-2539-41af-aaa4-ccfb6d02f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature-outlier score correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_correlations.plot(kind='bar')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Absolute Correlation with Outlier Score')\n",
    "plt.title('Feature-Outlier Score Correlations')\n",
    "plt.show()\n",
    "\n",
    "# I computed correlation scores of each feature against their outlier score\n",
    "# and some features to consider/ reaffrim choices about feature selection as they are correlated highly with outlier scores\n",
    "# however, this could be just be a guide as outliers may still be truly representative of data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fb3a0-cce4-45da-84e7-91357f1206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also remove outliers from treated data\n",
    "\n",
    "index_of_outliers = [36,62,77]\n",
    "\n",
    "b_cancer_treated = b_cancer_treated.drop(index=index_of_outliers)\n",
    "\n",
    "print(b_cancer_treated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af55ae-7eae-45de-8c44-7fa8f8b4b2c3",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e84d55-73a5-4eb2-b62d-c6ede7be1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "X, y = b_cancer.drop(columns=['Grade']), b_cancer['Grade'] \n",
    "\n",
    "# split data into train & test sets: 70% training & 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "# confirm splitting\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d9bd4-ee4f-4ae2-aa88-93bd3f65304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split treated data under same parameters\n",
    "# ensure no names conflict \n",
    "seed = 5 \n",
    "np.random.seed(seed)\n",
    "X_treated, y_treated = b_cancer_treated.drop(columns=['Grade']), b_cancer_treated['Grade']\n",
    "\n",
    "# Split data into train and test set (70% training, 30% testing)\n",
    "X_train_treated, X_test_treated, y_train_treated, y_test_treated = train_test_split(X_treated, y_treated, test_size=0.3, random_state=1)\n",
    "\n",
    "# Confirm splitting\n",
    "print(X_train_treated.shape, X_test_treated.shape, y_train_treated.shape, y_test_treated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a727287-b9fd-4f4e-9dae-ab8caea0a4e1",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368dbf2-a4fe-40ac-808e-e5662bda91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first perform elastic net then RFECV on untreated data\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Create an instance of ElasticNetCV\n",
    "elastic_net = ElasticNetCV(cv=10, random_state=6) # set cv = 10 as this is standard practise and relatively small dataset\n",
    "\n",
    "# Fit the model on the training data\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_feature_indices = [i for i, coef in enumerate(elastic_net.coef_) if coef != 0]\n",
    "\n",
    "# Get the names of selected features\n",
    "selected_features = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20900ced-70d5-4078-a959-20fb89b0fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter X_train and X_test to include only selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74444d5d-f1d0-4b08-9162-4dd5b3feb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now RFECV\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Initialize the base model (the estimator)\n",
    "base_model = LogisticRegression()  # You can replace this with your preferred classifier\n",
    "\n",
    "# Initialize RFECV\n",
    "rfecv = RFECV(estimator=base_model, cv=10)  # Set cv as desired (number of folds for cross-validation)\n",
    "\n",
    "# Fit RFECV on the selected features and target variable\n",
    "rfecv.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the indices of selected features after RFECV\n",
    "selected_feature_indices_rfecv = rfecv.support_\n",
    "\n",
    "# Get the names of selected features after RFECV\n",
    "selected_features_rfecv = selected_features[selected_feature_indices_rfecv]\n",
    "\n",
    "# Print the selected features after RFECV\n",
    "print(\"Selected Features after RFECV:\")\n",
    "print(selected_features_rfecv)\n",
    "\n",
    "# Print the optimal number of features selected\n",
    "print(\"Optimal number of features selected:\", rfecv.n_features_)\n",
    "\n",
    "# Filter X_train_selected and X_test_selected to include only the selected features\n",
    "X_train_selected = X_train_selected[selected_features_rfecv]\n",
    "X_test_selected = X_test_selected[selected_features_rfecv]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c6d2d-70ea-4b12-8cbb-6cdb9e4eca87",
   "metadata": {},
   "source": [
    "### Same but now for treated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72c759-4c75-4b48-9fea-02a7e5f3ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for the treated data\n",
    "\n",
    "# Create an instance of ElasticNetCV\n",
    "elastic_net = ElasticNetCV(cv=10, random_state=6) # set cv = 10 as this is standard practise and relatively small dataset\n",
    "\n",
    "# Fit the model on the training data\n",
    "elastic_net.fit(X_train_treated, y_train_treated)\n",
    "\n",
    "# Get the indices of selected features\n",
    "treated_selected_feature_indices = [i for i, coef in enumerate(elastic_net.coef_) if coef != 0]\n",
    "\n",
    "# Get the names of selected features\n",
    "treated_selected_features = X_train_treated.columns[treated_selected_feature_indices]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Treated Data - Selected Features:\")\n",
    "print(treated_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d1040-98a4-4cdc-b130-da1dd95403d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter X_train and X_test to include only selected features\n",
    "X_train_treated_selected = X_train_treated[treated_selected_features]\n",
    "X_test_treated_selected = X_test_treated[treated_selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943ca47-daf6-4a52-9357-e7624e2c0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now RFECV\n",
    "# Initialize the base model (the estimator)\n",
    "base_model = LogisticRegression()  # You can replace this with your preferred classifier\n",
    "\n",
    "# Initialize RFECV\n",
    "rfecv = RFECV(estimator=base_model, cv=10)  # Set cv as desired (number of folds for cross-validation)\n",
    "\n",
    "# Fit RFECV on the filtered treated training data\n",
    "rfecv.fit(X_train_treated_selected, y_train_treated)\n",
    "\n",
    "# Get the indices of selected features after RFECV\n",
    "selected_feature_indices_rfecv = rfecv.support_\n",
    "\n",
    "# Get the names of selected features after RFECV\n",
    "selected_features_rfecv = treated_selected_features[selected_feature_indices_rfecv]\n",
    "\n",
    "# Print the selected features after RFECV\n",
    "print(\"Selected Features after RFECV:\")\n",
    "print(selected_features_rfecv)\n",
    "\n",
    "# Filter X_train_treated_selected and X_test_treated_selected to include only the selected features\n",
    "X_train_treated_selected = X_train_treated_selected[selected_features_rfecv]\n",
    "X_test_treated_selected = X_test_treated_selected[selected_features_rfecv]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757a075-6264-4037-872c-32cbe5538484",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38949f5b-3714-426a-a30b-5dfab4aadec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ae213-16ee-4751-80f1-7e00038e8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models on untreated data first \n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('SVM', SVC(kernel='poly')))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('BG', BaggingClassifier()))\n",
    "models.append(('ET', ExtraTreesClassifier()))\n",
    "models.append(('SGDC', SGDClassifier()))\n",
    "models.append(('NN', Perceptron()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('GB', GradientBoostingClassifier()))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('LGBM', LGBMClassifier()))\n",
    "models.append(('CatBoost', CatBoostClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
    "\n",
    "# sort the data and labels\n",
    "X = X_train_selected\n",
    "y = y_train\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "print('Calculating metrics')\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models:\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Make predictions on the training data\n",
    "    predictions = model.predict(X_train_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_mean = accuracy_score(y_train, predictions)\n",
    "    precision_mean = precision_score(y_train, predictions)\n",
    "    recall_mean = recall_score(y_train, predictions)\n",
    "    f1_mean = f1_score(y_train, predictions)\n",
    "    \n",
    "    # Append the metrics to the data list\n",
    "    data.append({'Model': name, 'Accuracy_mean': accuracy_mean,\n",
    "                 'Precision_mean': precision_mean,\n",
    "                 'Recall_mean': recall_mean,\n",
    "                 'F1_mean': f1_mean})\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "model_metrics_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(model_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4141eb-4519-4ffe-bbab-698824dcd65d",
   "metadata": {},
   "source": [
    "Models appear to be overfitting as they produce perfect scores in their metrics.\n",
    "This indicates they have learnt the data exactly and will not generalise well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6629e4a-c95a-46f2-9858-f4383dceec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CROSS VALIDATION BASED TRAINING TO ADDRESS OVERFITTING \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models:\n",
    "    # Perform cross-validation and compute scores for the current model\n",
    "    cv_results = cross_validate(model, X_train_selected, y_train, cv=10,\n",
    "                                scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "    # Calculate the mean of cross-validation scores for each metric\n",
    "    accuracy_mean = np.mean(cv_results['test_accuracy'])\n",
    "    precision_mean = np.mean(cv_results['test_precision'])\n",
    "    recall_mean = np.mean(cv_results['test_recall'])\n",
    "    f1_mean = np.mean(cv_results['test_f1'])\n",
    "\n",
    "    # Append the cross-validation scores for the current model to the data list\n",
    "    data.append({'Model': name,\n",
    "                 'Accuracy_cv_mean': accuracy_mean,\n",
    "                 'Precision_cv_mean': precision_mean,\n",
    "                 'Recall_cv_mean': recall_mean,\n",
    "                 'F1_cv_mean': f1_mean})\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "cv_model_metrics_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(cv_model_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1dbe6-9a7d-4577-857b-1e8992a1b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the averages of all the mean performance metriccs by model  \n",
    "# Calculate the average of each numerical value in a row and add a column with those averages for each model\n",
    "cv_model_metrics_df['Mean'] = cv_model_metrics_df.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# plot in descending order\n",
    "cv_model_metrics_df_sorted = cv_model_metrics_df.sort_values(by='Mean', ascending=False)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(cv_model_metrics_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9fc6b-427a-40fc-9c10-499a1aac40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cv- average metrics for untreated data as bar chart \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get 17 distinct colors from the 'tab20' color palette\n",
    "colors = plt.cm.tab20.colors[:17]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(cv_model_metrics_df_sorted['Model'], cv_model_metrics_df_sorted['Mean'], color=colors)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average of Performance Metrics - Cross Validation')\n",
    "plt.title('Average Performance by Model for Untreated Data')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Hide the legend\n",
    "plt.legend().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0377f85-2496-46c9-b946-93286fcaa852",
   "metadata": {},
   "source": [
    "Now do the same for treated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc02f4-dc75-46c0-9529-e2750c9f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create different list of trained models for treated data\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "# prepare models\n",
    "models_treated = []\n",
    "models_treated.append(('LR', LogisticRegression()))\n",
    "models_treated.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models_treated.append(('KNN', KNeighborsClassifier()))\n",
    "models_treated.append(('DT', DecisionTreeClassifier()))\n",
    "models_treated.append(('GNB', GaussianNB()))\n",
    "models_treated.append(('SVM', SVC(kernel='poly')))\n",
    "models_treated.append(('RF', RandomForestClassifier()))\n",
    "models_treated.append(('BG', BaggingClassifier()))\n",
    "models_treated.append(('ET', ExtraTreesClassifier()))\n",
    "models_treated.append(('SGDC', SGDClassifier()))\n",
    "models_treated.append(('NN', Perceptron()))\n",
    "models_treated.append(('XGB', XGBClassifier()))\n",
    "models_treated.append(('GB', GradientBoostingClassifier()))\n",
    "models_treated.append(('AB', AdaBoostClassifier()))\n",
    "models_treated.append(('MLP', MLPClassifier()))\n",
    "models_treated.append(('LGBM', LGBMClassifier()))\n",
    "models_treated.append(('CatBoost', CatBoostClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "# evaluate each model in turn\n",
    "results_t = []\n",
    "names_t = []\n",
    "metrics_t = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scoring_t = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
    "\n",
    "# sort the data and labels\n",
    "X_t = X_train_treated_selected\n",
    "y_t = y_train_treated\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data_treated = []\n",
    "\n",
    "print('Calculating metrics for treated data')\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models_treated:\n",
    "    # Fit the model on the treated training data\n",
    "    model.fit(X_train_treated_selected, y_train_treated)\n",
    "    \n",
    "    # Make predictions on the treated training data\n",
    "    predictions_treated = model.predict(X_train_treated_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_mean_treated = accuracy_score(y_train_treated, predictions_treated)\n",
    "    precision_mean_treated = precision_score(y_train_treated, predictions_treated)\n",
    "    recall_mean_treated = recall_score(y_train_treated, predictions_treated)\n",
    "    f1_mean_treated = f1_score(y_train_treated, predictions_treated)\n",
    "    \n",
    "    # Append the metrics to the data list\n",
    "    data_treated.append({'Model': name, 'Accuracy_mean': accuracy_mean_treated,\n",
    "                 'Precision_mean': precision_mean_treated,\n",
    "                 'Recall_mean': recall_mean_treated,\n",
    "                 'F1_mean': f1_mean_treated})\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "model_metrics_treated_df = pd.DataFrame(data_treated)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(model_metrics_treated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c67bb-badf-49e6-8649-f88f81355617",
   "metadata": {},
   "source": [
    "Same issues with overfitting. Perform cross-validation to improve training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c8b68-5958-4272-b5a7-2ab9fb410f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models with cv, again to lessen overfitting of trained models\n",
    "# Initialize an empty list to store the data\n",
    "data_treated_cv = []\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models_treated:\n",
    "    # Perform cross-validation and compute scores for the current model\n",
    "    cv_results_treated = cross_validate(model, X_train_treated_selected, y_train_treated, cv=10,\n",
    "                                         scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "    # Calculate the mean of cross-validation scores for each metric\n",
    "    accuracy_mean_treated = np.mean(cv_results_treated['test_accuracy'])\n",
    "    precision_mean_treated = np.mean(cv_results_treated['test_precision'])\n",
    "    recall_mean_treated = np.mean(cv_results_treated['test_recall'])\n",
    "    f1_mean_treated = np.mean(cv_results_treated['test_f1'])\n",
    "\n",
    "    # Append the cross-validation scores for the current model to the data list\n",
    "    data_treated_cv.append({'Model': name,\n",
    "                            'Accuracy_cv_mean_treated': accuracy_mean_treated,\n",
    "                            'Precision_cv_mean_treated': precision_mean_treated,\n",
    "                            'Recall_cv_mean_treated': recall_mean_treated,\n",
    "                            'F1_cv_mean_treated': f1_mean_treated})\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "cv_model_metrics_treated_df = pd.DataFrame(data_treated_cv)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(cv_model_metrics_treated_df)\n",
    "\n",
    "# Append the averages of all the mean performance metrics by model\n",
    "# Calculate the average of each numerical value in a row and add a column with those averages for each model\n",
    "cv_model_metrics_treated_df['Mean_treated'] = cv_model_metrics_treated_df.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Plot in descending order\n",
    "cv_model_metrics_treated_df_sorted = cv_model_metrics_treated_df.sort_values(by='Mean_treated', ascending=False)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(cv_model_metrics_treated_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a44ea-a2c9-4d2d-9fcd-611afc84ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average metrics for treated data\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(cv_model_metrics_treated_df_sorted['Model'], cv_model_metrics_treated_df_sorted['Mean_treated'], color=colors)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average of Performance Metrics - Cross Validation')\n",
    "plt.title('Average Performance by Model for Treated Data')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Hide the legend\n",
    "plt.legend().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdb2d8-07da-4df3-8716-536f07dffcf7",
   "metadata": {},
   "source": [
    "## Validation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf768f3-e3de-45da-9fbc-a47faa75a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#untreated data\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "# evaluate more non treated data models on test data\n",
    "# Get the top performing models from the sorted DataFrame\n",
    "top_models = cv_model_metrics_df_sorted.head(4)  # select n \n",
    "\n",
    "# Initialize an empty list to store the evaluation metrics\n",
    "test_set_metrics = []\n",
    "\n",
    "# Loop through the top performing models\n",
    "for index, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    model = None\n",
    "    \n",
    "    # Find the corresponding model instance from the models list\n",
    "    for model_tuple in models:\n",
    "        if model_tuple[0] == model_name:\n",
    "            model = model_tuple[1]\n",
    "            break\n",
    "    \n",
    "    if model:\n",
    "        # Make predictions on the test set\n",
    "        predictions = model.predict(X_test_selected)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions)\n",
    "        auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "        # Append the metrics to the list\n",
    "        test_set_metrics.append({'Model': model_name,\n",
    "                                 'Accuracy': accuracy,\n",
    "                                 'Precision': precision,\n",
    "                                 'Recall': recall,\n",
    "                                 'F1 Score': f1,\n",
    "                                 'AUC': auc})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "test_set_metrics_df = pd.DataFrame(test_set_metrics)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(test_set_metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614a66f-1e0d-4cf9-b59c-d1b2809a7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages\n",
    "test_set_metrics_df['Mean'] = test_set_metrics_df.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(test_set_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6eb7a-e100-43a3-9cf2-3404326b885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################do the same for treated data##########################\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "top_models_treated = cv_model_metrics_treated_df_sorted.head(4)  # select n \n",
    "\n",
    "# Initialize an empty list to store the evaluation metrics for treated data\n",
    "test_set_metrics_treated = []\n",
    "\n",
    "# Loop through the top performing models for treated data\n",
    "for index, row in top_models_treated.iterrows():\n",
    "    model_name = row['Model']\n",
    "    model = None\n",
    "    \n",
    "    # Find the corresponding model instance from the models_treated list\n",
    "    for model_tuple in models_treated:\n",
    "        if model_tuple[0] == model_name:\n",
    "            model = model_tuple[1]\n",
    "            break\n",
    "    \n",
    "    if model:\n",
    "        # Make predictions on the treated test set\n",
    "        predictions_treated = model.predict(X_test_treated_selected)\n",
    "\n",
    "        # Calculate evaluation metrics for treated data\n",
    "        accuracy_treated = accuracy_score(y_test_treated, predictions_treated)\n",
    "        precision_treated = precision_score(y_test_treated, predictions_treated)\n",
    "        recall_treated = recall_score(y_test_treated, predictions_treated)\n",
    "        f1_treated = f1_score(y_test_treated, predictions_treated)\n",
    "        auc_treated = roc_auc_score(y_test_treated, predictions_treated)\n",
    "\n",
    "        # Append the metrics to the list for treated data\n",
    "        test_set_metrics_treated.append({'Model': model_name,\n",
    "                                         'Accuracy': accuracy_treated,\n",
    "                                         'Precision': precision_treated,\n",
    "                                         'Recall': recall_treated,\n",
    "                                         'F1 Score': f1_treated,\n",
    "                                         'AUC': auc_treated})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for treated data\n",
    "test_set_metrics_df_treated = pd.DataFrame(test_set_metrics_treated)\n",
    "\n",
    "# Calculate averages for treated data\n",
    "test_set_metrics_df_treated['Mean'] = test_set_metrics_df_treated.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Display the DataFrame with the new column for treated data\n",
    "print(test_set_metrics_df_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd685d8-fcd8-47b7-935e-145bb3883ec7",
   "metadata": {},
   "source": [
    "UNTREATED PROVIDES BETTER MODELS. LESS OVERFITTING TO TRAINING DATASET. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c06924-3839-46b9-bb49-327dc31dbc98",
   "metadata": {},
   "source": [
    "## Ensemble Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755328b8-6335-439d-b6ab-3bc8c05fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract trained models from list\n",
    "trained_mlp_model = models[10][1]  # MLP is the 11th model in the list of models\n",
    "trained_lda_model = models[1][1]   # LDA is the 2nd model in the list of models\n",
    "trained_lr_model = models[0][1]   # LR is the 1st model in the list of models\n",
    "\n",
    "# import module\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "seed = 6 \n",
    "\n",
    "np.random.seed(seed)\n",
    "# Define the ensemble models\n",
    "ensemble_models = [\n",
    "    ('MLP', trained_mlp_model),  # trained_mlp_model is the multi layer percep Classifier\n",
    "    ('RF', trained_lda_model),  #  trained_lda_model is the lineardiscriminantClassifier\n",
    "    ('LR', trained_lr_model)   #  trained_lr_model is the logisticregClassifier\n",
    "]\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=ensemble_models, voting='hard')  # 'hard' for majority voting\n",
    "\n",
    "# Fit the Voting Classifier on the training data\n",
    "voting_clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "ensemble_predictions = voting_clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the ensemble performance\n",
    "e_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "e_precision = precision_score(y_test, ensemble_predictions)\n",
    "e_recall = recall_score(y_test, ensemble_predictions)\n",
    "e_f1 = f1_score(y_test, ensemble_predictions)\n",
    "e_auc = roc_auc_score(y_test, ensemble_predictions)\n",
    "\n",
    "# Store the metrics in a dictionary\n",
    "ensemble_metrics = {\n",
    "    'Accuracy': e_accuracy,\n",
    "    'Precision': e_precision,\n",
    "    'Recall': e_recall,\n",
    "    'F1 Score': e_f1,\n",
    "    'AUC': e_auc\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ensemble_metrics_df = pd.DataFrame([ensemble_metrics])\n",
    "\n",
    "# Calculate the average of each numerical value in a row and add a column with those averages\n",
    "ensemble_metrics_df['Mean'] = ensemble_metrics_df.mean(axis=1)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(ensemble_metrics_df)\n",
    "# ensemble has inferior metrics to multi layer perceptron alone for untreated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a216b67-334c-4947-9bf9-4a54e1eb501c",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccddd36-8669-4ed1-8b39-87cee2bcf22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Hyperparameter tuning of MLP model for non-treated data\n",
    "seed = 6 \n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define the parameter grid for MLP hyperparameter tuning\n",
    "mlp_param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],  # Size of hidden layers\n",
    "    'activation': ['relu', 'logistic'],  # Activation function for the hidden layer\n",
    "    'solver': ['adam', 'sgd'],  # Solver for weight optimization\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularization parameter\n",
    "    'learning_rate': ['constant', 'adaptive']  # Learning rate schedule\n",
    "}\n",
    "\n",
    "# Initialize the MLPClassifier model\n",
    "mlp_model = MLPClassifier(random_state=seed)\n",
    "\n",
    "# Initialize the RandomizedSearchCV object for MLP\n",
    "mlp_random_search = RandomizedSearchCV(estimator=mlp_model, param_distributions=mlp_param_dist, \n",
    "                                       n_iter=10, cv=5, scoring='accuracy', random_state=seed)\n",
    "\n",
    "# Perform the randomized search on the training data\n",
    "mlp_random_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best parameters found by the randomized search for MLP\n",
    "mlp_best_params = mlp_random_search.best_params_\n",
    "\n",
    "# Get the best MLP model found by the randomized search\n",
    "best_mlp_model = mlp_random_search.best_estimator_\n",
    "\n",
    "# Get predicted probabilities for MLP\n",
    "mlp_probs = best_mlp_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC score for MLP\n",
    "mlp_fpr, mlp_tpr, _ = roc_curve(y_test, mlp_probs)\n",
    "mlp_auc = auc(mlp_fpr, mlp_tpr)\n",
    "\n",
    "# Print MLP's best parameters and metrics\n",
    "print(\"MLP Best Parameters:\", mlp_best_params)\n",
    "print(\"MLP AUC:\", mlp_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380bb1f-b6b3-4ad1-86ef-97dce5816c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning of LR\n",
    "\n",
    "from scipy.stats import uniform\n",
    "seed = 6 \n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning of LR\n",
    "lr_param_dist = {\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'C': uniform(loc=0, scale=4)  # Inverse of regularization strength\n",
    "}\n",
    "\n",
    "# Initialize the LogisticRegression model\n",
    "lr_model = LogisticRegression(random_state=seed)\n",
    "\n",
    "# Initialize the RandomizedSearchCV object for LR\n",
    "lr_random_search = RandomizedSearchCV(estimator=lr_model, param_distributions=lr_param_dist, \n",
    "                                      n_iter=10, cv=5, scoring='accuracy', random_state=seed)\n",
    "\n",
    "# Perform the randomized search on the training data\n",
    "lr_random_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best parameters found by the randomized search for LR\n",
    "lr_best_params = lr_random_search.best_params_\n",
    "\n",
    "# Get the best LR model found by the randomized search\n",
    "best_lr_model = lr_random_search.best_estimator_\n",
    "\n",
    "# Get predicted probabilities for LR\n",
    "lr_probs = best_lr_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC score for LR\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "lr_auc = auc(lr_fpr, lr_tpr)\n",
    "\n",
    "# Print LR's best parameters and metrics\n",
    "print(\"LR Best Parameters:\", lr_best_params)\n",
    "print(\"LR AUC:\", lr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac282e-4243-4a49-88e7-e0e15a13ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning of LDA\n",
    "seed = 6 \n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning of LDA\n",
    "lda_param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Initialize the GridSearchCV object for LDA\n",
    "lda_grid_search = GridSearchCV(estimator=lda_model, param_grid=lda_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on the training data for LDA\n",
    "lda_grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best parameters found by the grid search for LDA\n",
    "lda_best_params = lda_grid_search.best_params_\n",
    "\n",
    "# Get the best LDA model found by the grid search\n",
    "best_lda_model = lda_grid_search.best_estimator_\n",
    "\n",
    "# Get predicted probabilities for LDA\n",
    "lda_probs = best_lda_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC score for LDA\n",
    "lda_fpr, lda_tpr, _ = roc_curve(y_test, lda_probs)\n",
    "lda_auc = auc(lda_fpr, lda_tpr)\n",
    "\n",
    "# Print LDA's best parameters and metrics\n",
    "print(\"LDA Best Parameters:\", lda_best_params)\n",
    "print(\"LDA AUC:\", lda_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332f0d0-b006-4271-a3a9-3a3fb0ae211f",
   "metadata": {},
   "source": [
    "## Plot ROC curve for 3 best models with hyperparameters tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00cc4d-7b72-4f70-ac8f-1c3cafc0a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each model\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mlp_fpr, mlp_tpr, linestyle='-', label=f'MLP (AUC = {mlp_auc:.2f})')\n",
    "plt.plot(lr_fpr, lr_tpr, linestyle='-', label=f'LR (AUC = {lr_auc:.2f})')\n",
    "plt.plot(lda_fpr, lda_tpr, linestyle='-', label=f'LDA (AUC = {lda_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
